---
title: "「PaperNote」 2410_LAPA: Latent Action Pretraining From Videos"
author: Hadrian X
date: 2025-08-14 10:55:00 +0800
categories:
  - PaperNote
  - Robotics
tags:
  - paper_reading
  - robotics
pin: false
math: true
mermaid: false
---
## LAPA
- [Latent Action Pretraining From Videos](https://arxiv.org/pdf/2410.11758)
- [project page](https://latentactionpretraining.github.io/)

-----

## Abstract
![image.png](https://cdn.jsdelivr.net/gh/hadrian0612/image/images/20250814110242324.png)
- Latent Action Pretraining
- the first unsupervised method for pertraining Vision-Language-Action models without ground-truth robot action labels
- 人类遥操采集有动作标签的数据极大程度限制了可用数据来源与规模（1. action lable 2. domain gap）
- 首先训练一个基于VQ-VAE的动作量化模型学习图像帧之间的离散潜在动作
- 再训练一个latent VLA，使其能从观测数据和任务描述中预测潜在动作
- 最后通过小规模机器操作数据微调VLA实现从潜在动作到机器人实际动作的映射

## Method
![image.png](https://cdn.jsdelivr.net/gh/hadrian0612/image/images/20250814110358520.png)
(1) **Latent Action Quantization** 潜在动作量化​​：我们首先以完全无监督的方式，利用VQ-VAE目标函数学习离散潜在动作
​​(2) **Latent Pretraining** 潜在预训练​​：训练视觉语言模型（VLM）预测潜在动作，本质上是行为克隆。预训练完成后，我们在少量动作标注轨迹上对LAPA进行微调，将潜在空间映射到末端执行器的增量动作空间。

- ChatGPT类模型的成功在于其可以利用大量的无需标签的数据
### Latent Action Quantization
- 参照[Genie](https://arxiv.org/pdf/2402.15391) 的方法训练了一个Latent Action Quantization Model 以完全无监督的方式学习 latent actions
- Latent Action Quantization Model 用的 encoder-decoder 架构
	- encoder 输入是当前帧$x_t$ 与未来帧$x_{t+H}$ 输出latent action $z_t$ 
	- decoder 输入是当前$x_t$  与latent action $z_t$  要重建未来帧$z_{t+H}$ 

### Latent Pretraining
- 使用潜在动作量化模型的编码器作为逆动力学模型，在给定帧xₜ₊₁的条件下，为所有帧xₜ标注潜在动作zₜ
- 随后，我们通过预训练的视觉语言模型（VLM），在给定视频片段语言指令和当前图像xₜ的条件下，预测潜在动作zₜ，以此进行动作预训练。

### Action Finetuning
- 经过预训练、用于预测潜在动作的视觉语言动作模型（VLA）无法直接在真实机器人上执行，因为潜在动作并非真实的末端执行器增量动作或关节动作。
- 为实现潜在动作到真实机器人动作的映射，我们在少量包含真实动作标注（末端执行器增量动作）的轨迹数据上对LAPA模型进行微调。
## Summary
- 本文提出的训练框架解决了海量的数据因为需要动作标签和domain gap难以用于机器人训练的问题
- 首先训练一个潜在动作量化模型，输入两帧图像输出期间动作的潜在编码
- 而后用动作潜在编码做监督训练VLM，输入当前动作，视频片段与语言指示输出潜在动作
- 最后用少量有动作标签的数据将模型适配到具体机器人上，即将动作潜在编码对齐到真机动作
## Appendix